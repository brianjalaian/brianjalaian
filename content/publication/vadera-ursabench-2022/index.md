---
title: 'URSABench: A System for Comprehensive Benchmarking of Bayesian Deep Neural
  Network Models and Inference Methods'
authors:
- admin
- Meet Vadera
- Jinyang Li
- Adam Cobb
- Tarek Abdelzaher
- Benjamin Marlin
date: '2022-04-01'
publishDate: '2024-10-06T03:28:00.675061Z'
publication_types:
- article-journal
publication: '*Proceedings of Machine Learning and Systems*'
links:
- name: URL
  url: 
    https://proceedings.mlsys.org/paper_files/paper/2022/hash/4a420924d20bc025ebb37849169e6ebd-Abstract.html

url_code: https://github.com/reml-lab/URSABench
url_pdf: https://proceedings.mlsys.org/paper_files/paper/2022/file/4a420924d20bc025ebb37849169e6ebd-Paper.pdf
abstract: While deep learning methods continue to improve in predictive accuracy on a wide range of application domains, significant issues remain with other aspects of their performance, including their ability to quantify uncertainty and their robustness. Recent advances in approximate Bayesian inference hold significant promise for addressing these concerns, but the computational scalability of these methods can be problematic when applied to large-scale models. In this paper, we present URSABench (the Uncertainty, Robustness, Scalability, and Accuracy Benchmark), an open-source suite of models, inference methods, tasks and benchmarking tools. URSABench supports comprehensive assessment of Bayesian deep learning models and approximate Bayesian inference methods, with a focus on classification tasks performed both on server and edge GPUs.

summary: URSABench is an open-source benchmarking suite for assessing Bayesian deep learning models and inference methods, focusing on uncertainty quantification, robustness, scalability, and accuracy in classification tasks for both server and edge GPUs.

tags:
  - Bayesian Deep Learning
  - Uncertainty Quantification
  - Robustness
  - Scalability
  - Benchmarking 

featured: true

---
